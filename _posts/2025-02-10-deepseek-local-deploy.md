---
layout: post
title: è½»é‡çº§éƒ¨ç½²ä½é…ç‰ˆDeepseek R1è’¸é¦æ¨¡å‹
date: 2025-02-10 16:39:56
mathjax: false
disqus: true
categories: AI
---

> æœ‰å¤šå°‘äººå·¥å°±æœ‰å¤šå°‘æ™ºèƒ½

æ˜¥èŠ‚å‡æœŸDeepseekå†²ä¸Šçƒ­æœï¼Œå› ä¸ºåª²ç¾OpenAI-o1çš„èƒ½åŠ›è€Œæˆä¸ºå›½å†…è¿½æ§çš„æœ€å¼ºå¤§æ¨¡å‹ã€‚åˆå› ä¸ºä½¿ç”¨äº†è´´å§æš´èºè€å“¥çš„è®­ç»ƒæ•°æ®ä½¿å¾—å®ƒåœ¨ä¸­æ–‡æ€¼äººé¢†åŸŸé¥é¥é¢†å…ˆã€‚

![](../../../../assets/images/11739179782_.pic.jpg)

ä¸‹é¢å°±æ¥è¯•è¯•æœ¬åœ°è¿è¡Œä¸€ä¸‹ã€‚

### é€‰æ‹©æ¨¡å‹
éœ€è¦æ ¹æ®ç¡¬ä»¶æ¥é€‰æ‹©ï¼Œæ³¨æ„æ˜¾å¡å‹å·å’Œç¡¬ç›˜ç©ºé—´ã€‚é‡åŒ–ç‰ˆæœ¬ä¼šä½¿ç”¨æ›´å°‘çš„æ˜¾å­˜ï¼Œä½†ä¼šæŸå¤±ä¸€äº›æ€§èƒ½ã€‚
è¿˜æœ‰ä¸€äº›è’¸é¦åçš„æ¨¡å‹ï¼Œæ‹¥æœ‰æ›´å°çš„å‚æ•°é‡ã€‚
> ä»¥ä¸‹æ˜¯DeepSeek R1ç³»åˆ—æ¨¡å‹çš„æ˜¾å­˜éœ€æ±‚åŠé‡åŒ–åçš„æ˜¾å­˜ä¼°ç®—ï¼ŒåŸºäºHuggingfaceæ¨¡å‹é¡µé¢çš„å…¬å¼€ä¿¡æ¯æ•´ç†ï¼š
> 
> | æ¨¡å‹åç§°                                     | Huggingfaceé“¾æ¥                                                                 | æ¨¡å‹å¤§å° | åŸå§‹æ˜¾å­˜ (GB) | int8é‡åŒ–æ˜¾å­˜ (GB) | int4é‡åŒ–æ˜¾å­˜ (GB) |
> |---------------------------------------------|--------------------------------------------------------------------------------|----------|----------------|-------------------|-------------------|
> | DeepSeek-R1                                 | [é“¾æ¥](https://huggingface.co/deepseek-ai/DeepSeek-R1)                         | 7B       | 14             | 7                 | 3.5               |
> | DeepSeek-R1-Distill-Llama-70B               | [é“¾æ¥](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)       | 70B      | 140            | 70                | 35                |
> | DeepSeek-R1-Zero                            | [é“¾æ¥](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)                    | 1.3B     | 2.6            | 1.3               | 0.65              |
> | DeepSeek-R1-Distill-Qwen-7B                 | [é“¾æ¥](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)         | 7B       | 14             | 7                 | 3.5               |
> | DeepSeek-R1-Distill-Qwen-32B                | [é“¾æ¥](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)        | 32B      | 64             | 32                | 16                |
> | DeepSeek-R1-Distill-Qwen-14B                | [é“¾æ¥](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)        | 14B      | 28             | 14                | 7                 |
> | DeepSeek-R1-Distill-Qwen-1.5B               | [é“¾æ¥](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)       | 1.5B     | 3              | 1.5               | 0.75              |
> | DeepSeek-R1-Distill-Llama-8B                | [é“¾æ¥](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)        | 8B       | 16             | 8                 | 4                 |
> 
> #### è¯´æ˜ï¼š
> 1. **æ˜¾å­˜è®¡ç®—é€»è¾‘**ï¼š
>    - **åŸå§‹æ˜¾å­˜**ï¼šæ¨¡å‹å‚æ•°é‡ Ã— 2ï¼ˆFP16/BF16æ ¼å¼ï¼Œæ¯å‚æ•°å 2å­—èŠ‚ï¼‰ã€‚
>    - **int8é‡åŒ–**ï¼šæ¨¡å‹å‚æ•°é‡ Ã— 1ï¼ˆæ¯å‚æ•°å 1å­—èŠ‚ï¼‰ã€‚
>    - **int4é‡åŒ–**ï¼šæ¨¡å‹å‚æ•°é‡ Ã— 0.5ï¼ˆæ¯å‚æ•°å 0.5å­—èŠ‚ï¼‰ã€‚
>    - å®é™…éƒ¨ç½²æ—¶éœ€é¢å¤–è€ƒè™‘æ¿€æ´»å€¼ã€ä¸­é—´è®¡ç®—ç»“æœçš„æ˜¾å­˜å ç”¨ï¼ˆçº¦ä¸ºæƒé‡çš„1.2-1.5å€ï¼‰ã€‚
> 
> 2. **æ¨¡å‹å‚æ•°æ¥æº**ï¼š
>    - æ¨¡å‹åç§°ä¸­æ˜ç¡®åŒ…å«å‚æ•°é‡ï¼ˆå¦‚`70B`ã€`7B`ï¼‰çš„ï¼Œä»¥åç§°æ ‡æ³¨çš„å‚æ•°é‡ä¸ºå‡†ã€‚
>    - `DeepSeek-R1`å’Œ`DeepSeek-R1-Zero`çš„å‚æ•°é‡å‚è€ƒHuggingfaceæ¨¡å‹é…ç½®æ–‡ä»¶ï¼ˆ`config.json`ï¼‰åŠå®˜æ–¹æŠ€æœ¯æŠ¥å‘Šã€‚
> 
> 3. **æ³¨æ„äº‹é¡¹**ï¼š
>   - é‡åŒ–åçš„æ˜¾å­˜ä»…ä¸ºç†è®ºå€¼ï¼Œå®é™…éƒ¨ç½²éœ€ç»“åˆæ¡†æ¶æ”¯æŒï¼ˆå¦‚AWQã€GPTQç­‰ï¼‰ã€‚
>   - è‹¥éœ€å®Œæ•´æ¨ç†ï¼ˆå«æ¿€æ´»å€¼ï¼‰ï¼Œæ˜¾å­˜éœ€æ±‚éœ€åœ¨è¡¨æ ¼åŸºç¡€ä¸Šé¢å¤–å¢åŠ çº¦20%-50%ã€‚
> 
> å»ºè®®é€šè¿‡`bitsandbytes`æˆ–`vLLM`ç­‰å·¥å…·å®ç°é‡åŒ–éƒ¨ç½²ï¼Œå…·ä½“æ˜¾å­˜å ç”¨å¯èƒ½å› æ¡†æ¶ä¼˜åŒ–è€Œå¼‚ã€‚

### ä¸‹è½½æƒé‡
å¸¸ç”¨çš„æœ‰ä¸‰ç§æ–¹å¼ï¼šç½‘é¡µä¸‹è½½ã€gitä¸‹è½½å’Œpythonä»£ç ä¸‹è½½ã€‚**ç”±äºæƒé‡æ–‡ä»¶è¾ƒå¤§ï¼Œè¯·æ³¨æ„ç¡¬ç›˜å‰©ä½™ç©ºé—´ï¼ï¼ï¼**
1. ç½‘é¡µä¸‹è½½

    é€šè¿‡è®¿é—®huggingfaceçš„ä»“åº“ç½‘é¡µï¼Œç‚¹å‡»å¯¹åº”çš„æƒé‡æ–‡ä»¶ä¸‹è½½ã€‚å¦‚æœä»£ç†è¾ƒå·®ï¼Œå¯ä»¥ä½¿ç”¨[å›½å†…é•œåƒç«™](https://hf-mirror.com)ã€‚

2. gitä¸‹è½½

   æƒé‡å¼€æºå¿…å¤‡ç½‘ç«™[huggingface](https://huaggingface.co)ï¼Œæ—©åœ¨Stable Diffusionç«èµ·æ¥æ—¶å°±è¢«å¢™ï¼Œä¸‹è½½æƒé‡åªèƒ½é€šè¿‡ä»£ç†æ¥è¿›è¡Œã€‚è¿™é‡Œé‡‡ç”¨äº†git cloneçš„æ–¹å¼ä¸‹è½½ã€‚é¦–å…ˆå®‰è£…ä¸€ä¸‹git lfsï¼Œç”¨æ¥ä¸‹è½½å¤§æ–‡ä»¶æƒé‡ã€‚
    ```bash
    # Make sure you have git-lfs installed (https://git-lfs.com)
    git lfs install
    ```
    æ¥ä¸‹æ¥ä¸‹è½½æƒé‡ä»“åº“ï¼Œè®°å¾—æå‰æŒ‚ä¸Šä»£ç†
    ```bash
    export https_proxy="http://ä½ çš„ä»£ç†åœ°å€:ç«¯å£"
    git clone https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B
    ```
    å¦‚æœç½‘ç»œä¸ä½³ä¸­é€”ä¸‹è½½å¤±è´¥å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å†æ¬¡å°è¯•
    ```bash
    cd DeepSeek-R1-Distill-Qwen-14B
    git lfs pull
    ```
    å¦‚æœè¿˜æ˜¯ä¸è¡Œï¼Œå»ºè®®æ£€æŸ¥ä»£ç†æˆ–è€…æ›´æ¢é•œåƒç«™ç‚¹ä¸‹è½½ï¼Œä¾‹å¦‚ï¼š
    ```bash
    git clone https://hf-mirror.com/deepseek-ai/DeepSeek-R1
    ```
    å®Œæˆä¹‹åå°±å¯ä»¥å‡†å¤‡è¿è¡Œè¯•è¯•äº†ã€‚

3. pythonä»£ç ä¸‹è½½

    é¦–å…ˆå®‰è£…åº“huggingface_hub:`pip install huggingface_hub`ã€‚ç„¶åæ–°å»ºä¸€ä¸ªpythonæ–‡ä»¶`download.py`ï¼Œå†™å…¥ä»¥ä¸‹å†…å®¹ï¼š
    ```python
    from huggingface_hub import hf_hub_download
    import os
    os.environ["HTTP_PROXY"] = "http://ä»£ç†åœ°å€:ç«¯å£"
    os.environ["HTTPS_PROXY"] = "http://ä»£ç†åœ°å€:ç«¯å£"

    # ä¸‹è½½å¤§æ–‡ä»¶
    file_path = hf_hub_download(
    repo_id="deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",  # æ¨¡å‹åç§°
    filename="model-00004-of-000004.safetensors",  # æ–‡ä»¶å
    cache_dir="./weights/DeepSeek-R1-Distill-Qwen-14B",  # ç¼“å­˜ç›®å½•
    force_download=True,  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
    )
    print(f"æ–‡ä»¶å·²ä¸‹è½½åˆ°: {file_path}")
    ```
    è¿è¡Œå³å¯ä¸‹è½½ã€‚

### æ­å»ºWebæ¥å£
ä¸»è¦åˆ†ä¸ºå‰ç«¯å’Œåç«¯é¡µé¢ï¼Œé¦–å…ˆæ¥çœ‹åç«¯ã€‚å› ä¸ºè¿™é‡Œåªæ˜¯è½»é‡çº§ä½¿ç”¨ï¼Œå°±ä¸ç”¨å¾ˆå¤šå·²æœ‰çš„å¤§æ¡†æ¶æ¥åšäº†ã€‚

**1. åç«¯æ¥å£**

é¦–å…ˆå®‰è£…ä¾èµ–
```bash
#pytorch è‡ªè¡Œæ ¹æ®å®˜ç½‘ç‰ˆæœ¬å®‰è£…
pip install fastapi uvicorn transformers accelerate sse_starlette
```
ç„¶åç¼–è¾‘æ¨¡å‹åŠ è½½ä¸æ¨ç†ä»£ç ï¼š
```python
from fastapi import FastAPI, Query
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
import torch
from sse_starlette.sse import EventSourceResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
import os
from threading import Thread


# å®šä¹‰è¯·æ±‚å‚æ•°æ ¼å¼
class Request(BaseModel):
    prompt: str
    max_length: int = 512
    temperature: float = 0.7
    top_p: float = 0.9

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "./weights/DeepSeek-R1-Distill-Qwen-14B"  # æ›¿æ¢ä¸ºå®é™…è·¯å¾„
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,  # ä½¿ç”¨ BF16 èŠ‚çœæ˜¾å­˜
    device_map="auto",           # è‡ªåŠ¨åˆ†é… GPU/CPU
    low_cpu_mem_usage=True
).eval()

# åˆ›å»º FastAPI å®ä¾‹
app = FastAPI()

# å…è®¸æ‰€æœ‰æ¥æºçš„è·¨åŸŸè¯·æ±‚ï¼ˆç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶åŸŸåï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["POST", "GET"],
    allow_headers=["*"],
)

# ä¿ç•™ä¹‹å‰çš„æ¨¡å‹åŠ è½½å’Œ /generate æ¥å£ä»£ç 

'''@app.post("/generate")
async def generate_text(request: Request):
    # ç¼–ç è¾“å…¥
    inputs = tokenizer(request.prompt, return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆæ–‡æœ¬
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=request.max_length,
            temperature=request.temperature,
            top_p=request.top_p,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # è§£ç ç»“æœ
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"response": response}'''

@app.get("/", response_class=FileResponse)
async def read_root():
    # å‡è®¾ index.html æ–‡ä»¶ä½äºå½“å‰ç›®å½•
    file_path = os.path.join(os.path.dirname(__file__), "index.html")
    return FileResponse(file_path)

@app.get("/stream-generate")
async def stream_generate(
    prompt: str = Query(..., description="è¾“å…¥æç¤ºè¯­"),
    max_length: int = Query(512, gt=0, le=2048),
    temperature: float = Query(0.7, ge=0.0, le=2.0)
):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, decode_kwargs={"errors": "ignore"})
    
    def event_generator():
        #yield tokenizer.decode(streamer, skip_special_tokens=True)
        thread = Thread(target=model.generate, kwargs={**inputs, "streamer": streamer, "max_new_tokens": max_length, "temperature": temperature, "pad_token_id":tokenizer.eos_token_id})
        thread.start()
        for text in streamer:
            yield {"data": text}
        yield {"data": "[DONE]"}
    
    return EventSourceResponse(event_generator())
```
å°†ä»¥ä¸Šå†…å®¹æ”¾å…¥web.pyï¼Œç„¶åè¿è¡Œ`uvicorn app:app --host 0.0.0.0 --port ä½ çš„ç«¯å£`å³å¯å¯åŠ¨åç«¯ã€‚ä»£ç æ•´ä½“ä½¿ç”¨äº†FASTAPI+transformersçš„æ¡†æ¶ã€‚`stream_generate`è·¯å¾„æ¨¡ä»¿äº†chatgptæµå¼ç”Ÿæˆï¼Œå¯åŠ¨äº†ä¸€ä¸ªçº¿ç¨‹è¿è¡Œå¤§æ¨¡å‹ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶å°†ç”Ÿæˆçš„æ–‡å­—æ”¾å…¥streamerã€‚æ ¹è·¯å¾„è¿”å›`index.html`ï¼Œå°†åœ¨ä¸‹é¢å‰ç«¯éƒ¨åˆ†è¯´æ˜ã€‚`generate`è·¯å¾„åˆ™æ˜¯ç›´æ¥è¿”å›å¤§æ¨¡å‹ç»“æœ
ï¼Œå¹³æ—¶ä¸å¤ªç”¨ï¼Œæ‰€ä»¥æ³¨é‡Šäº†ã€‚

**2. å‰ç«¯æ¥å£**
ä¸ºäº†ç®€å•èµ·è§ï¼Œè¿™é‡Œåªç¼–å†™äº†ä¸€ä¸ªhtmlæ–‡ä»¶ï¼Œå¯ä»¥é‡‡ç”¨å…¶ä»–æ–¹å¼æ¥å¢åŠ æ›´å¤šåŠŸèƒ½ã€‚
```html
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>DeepSeek-R1-Distill-Qwen-14B å¯¹è¯æœåŠ¡</title>
    <style>
        body { max-width: 800px; margin: 20px auto; padding: 20px; font-family: Arial, sans-serif; }
        #input { width: 100%; height: 100px; margin: 10px 0; padding: 10px; }
        #output { white-space: pre-wrap; background: #f5f5f5; padding: 10px; border-radius: 5px; }
        button { padding: 10px 20px; background: #007bff; color: white; border: none; border-radius: 5px; cursor: pointer; }
        button:hover { background: #0056b3; }
    </style>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.12.3/mark.min.js"></script>
    <script>
        marked.setOptions({
            highlight: function (code, lang) {
                return hljs.highlightAuto(code).value;
            },
            breaks: true
        });
    </script>
</head>
<body>
    <h1>DeepSeek Qwen 14B å¯¹è¯</h1>
    <textarea id="input" placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."></textarea>
    <button onclick="generateText()">æäº¤</button>
    <h3>æ¨¡å‹å›å¤ï¼š</h3>
    <div id="output">ç­‰å¾…è¾“å…¥...</div>

    <script>
        function generateText() {
            const input = document.getElementById("input").value;
            const outputDiv = document.getElementById("output");
            outputDiv.innerHTML = "";

            const prompt = encodeURIComponent(input);
            const url = `/stream-generate?prompt=${prompt}&max_length=2048&temperature=0.7`;

            const eventSource = new EventSource(url);

            eventSource.onmessage = (event) => {
                if (event.data === "[DONE]") {
                    eventSource.close();
                    const htmlContent = marked.parse(outputDiv.innerHTML);
                    outputDiv.innerHTML = htmlContent;
                    return;
                }
                outputDiv.innerHTML += event.data;
            };

            eventSource.onerror = (error) => {
                outputDiv.innerHTML = `é”™è¯¯: ${error.message}`;
                eventSource.close();
            };
        }
    </script>
</body>
</html>
```
å› ä¸ºEventSourceåªèƒ½ç”¨GETè¯·æ±‚ï¼Œæ‰€ä»¥å°†promptè¿›è¡ŒURIç¼–ç åæ”¾å…¥å‚æ•°ä¸­äº†ã€‚å¦å¤–è¿˜åŠ å…¥äº†markdownæ¸²æŸ“çš„åŠŸèƒ½ï¼Œæ˜¯åœ¨ç”Ÿæˆäº†æ‰€æœ‰å†…å®¹åæ‰è¿›è¡Œæ¸²æŸ“ã€‚
æœ€åæ”¾ä¸ŠDemoï¼š

![demo](../../../../assets/images/deepseek-web-demo.png)

---

ä»¥ä¸Šåªæ˜¯ä¸€ä¸ªç®€å•çš„éƒ¨ç½²è¿‡ç¨‹ï¼Œè¿˜æœ‰å¾ˆå¤šå¯ä»¥ä¼˜åŒ–çš„åœ°æ–¹ï¼Œå¦‚åŠ å…¥X-API-Keyã€ç”¨æˆ·ç®¡ç†ã€å†å²ä¼šè¯ç­‰
åŠŸèƒ½ã€‚å¦‚æœçœŸçš„éœ€è¦è€ƒè™‘ç”Ÿäº§åŠ›çš„è¯ï¼Œå»ºè®®ç›´æ¥çœ‹[vllm](https://github.com/vllm-project/vllm)ã€[ollama](https://ollama.com)ã€[Open-WebUI](https://openwebui.com)ç­‰é¡¹ç›®ã€‚æœ€åç¥å¤§å®¶å…ƒå®µèŠ‚å¿«ä¹ï¼

------
åŸºäºstreamlitæ›´æ–°äº†ä¸€ç‰ˆï¼Œä½†æ˜¯ç”±äºstreamlité€Ÿåº¦è¾ƒæ…¢ï¼Œæ¨èè¿˜æ˜¯ç”¨ä¹‹å‰fastapiçš„ç‰ˆæœ¬ï¼Œä¸‹é¢ç»™å‡ºéƒ¨åˆ†ä»£ç ï¼Œkeysè¯·è‡ªè¡Œæ·»åŠ 
```python
import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
import torch
from threading import Thread

def is_chinese_or_english(s):
    chinese_count = 0
    for c in s:
        if 0x4E00 <= ord(c) <= 0x9FFF or 0x3400 <= ord(c) <= 0x4DBF:
            chinese_count += 1
            break
    return (chinese_count > 0)

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "./weights/DeepSeek-R1-Distill-Qwen-14B"  # æ›¿æ¢ä¸ºå®é™…è·¯å¾„
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,  # ä½¿ç”¨ BF16 èŠ‚çœæ˜¾å­˜
    device_map="auto",           # è‡ªåŠ¨åˆ†é… GPU/CPU
    low_cpu_mem_usage=True
).eval()

auth_keys = ["ai"]

with st.sidebar:
    #st.title('Deepseek R1 Chatbot Demo')
    fast_api_key = st.text_input("FASTAPI Key", key="chatbot_api_key", type="password")
    
    st.subheader('Models and parameters')
    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=1.0, value=0.6, step=0.01)
    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.5, step=0.01)
    max_length = st.sidebar.slider('max_length', min_value=100, max_value=2048, value=1024, step=4)

    st.markdown('ğŸ“– Learn how to locally deploy deepseek in this [blog](https://blog.liuyc.uk/2025/02/10/deepseek-local-deploy/)!')


st.title("Deepseek Distill Qwen 14B Chatbot")
st.caption("ğŸš€ A Streamlit chatbot powered by Liu Yanchao")

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "æ‚¨å¥½ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨ï¼Ÿ(How can I help you?)"}]

for msg in st.session_state.messages:
    if msg["role"] == "assistant":
        st.chat_message(msg["role"],avatar='./bot-icon.png').write(msg["content"])
    else:
        st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    if not fast_api_key:
        st.info("Please add your FASTAPI key to continue.")
        st.stop()
    elif fast_api_key not in auth_keys:
        st.info("Your FASTAPI key is invalid.")
        st.stop()
    else:
        # è¾“å‡ºç”¨æˆ·è¾“å…¥çš„å¯¹è¯prompt
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)
        with st.chat_message("assistant",avatar='./bot-icon.png'):
            with st.spinner("Thinking..."):
                # æ ¼å¼åŒ–å¯¹è¯
                #formatted_dialogue = "\n".join([f"{turn['role']}: {turn['content']}" for turn in st.session_state.messages[-2:]])
                # æç¤ºè¯å·¥ç¨‹
                if is_chinese_or_english(prompt):
                    prompt = f"ä½ çš„ä»»åŠ¡æ˜¯å§‹ç»ˆä»¥â€œ<think>\nâ€å¼€å§‹ä½ çš„å›ç­”ï¼Œç„¶åæ¥ç€ç»™å‡ºä½ çš„ç†ç”±ã€‚æ¥ä¸‹æ¥æ˜¯è¾“å…¥ï¼š[{prompt}]"
                else:
                    prompt = f'Your task is to always start your response with "<think>\n" followed by your reasoning. Here is the input: [{prompt}]'
                # ç¼–ç è¾“å…¥
                inputs = tokenizer(prompt, return_tensors="pt",
                                    truncation=True, max_length=4096).to(model.device)
                placeholder = st.empty()
                
                streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, decode_kwargs={"errors": "ignore"})
                thread = Thread(target=model.generate, 
                                kwargs={**inputs, "streamer": streamer, 
                                                               "top_p": top_p, 
                                                               "max_new_tokens": max_length,
                                                               "temperature": temperature, 
                                                               "pad_token_id":tokenizer.eos_token_id})
                thread.start()

                
                full_response = '> '
                is_thinking = True
                for text in streamer:
                    if '<think>' in text:
                        is_thinking = True
                        full_response += "> "
                        placeholder.markdown(full_response)
                        continue
                    #print(text)
                    elif '</think>' in text:
                        is_thinking = False
                        #print("Think")
                        full_response += "\n"
                        placeholder.markdown(full_response)
                        continue
                    elif '<ï½œendâ–ofâ–sentenceï½œ>' in text:
                        #print("EOS")
                        full_response += "!\n"
                        break
                    
                    if is_thinking:
                        text = text.replace('\n', '\n> ')
                    
                    
                    full_response += text
                    placeholder.markdown(full_response)

                placeholder.markdown(full_response)

        st.session_state.messages.append({"role": "assistant", "content": full_response})
```
è¿è¡Œæ—¶ç›´æ¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å³å¯ï¼š
```bash
nohup streamlit run streamlit_app.py --server.port 23088 > streamlit.log 2>&1 &
```
